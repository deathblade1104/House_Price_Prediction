# -*- coding: utf-8 -*-
"""Housing_Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dtJ7yqy4ndv5KtA10DHzcqJo05xVqtGk
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import random
random.seed(0)

#importing dataset
dataset=pd.read_csv('housing.csv')

abc=list(dataset.columns)
for c in abc :
    print(c,dataset[c].nunique())
  
dataset.describe()
dataset.dtypes
dataset.isnull().mean()

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(dataset.iloc[:,[0,1,2,3,4,5,6,7,9]],
dataset.iloc[:,[8]],test_size=0.50,random_state=42)

def diagnostic_plots(df, variable):
    # function takes a dataframe (df) and
    # the variable of interest as arguments

    # define figure size
    plt.figure(figsize=(16, 4))

    # histogram
    plt.subplot(1, 3, 1)
    sns.distplot(df[variable], bins=30)
    plt.title('Histogram')

    # Q-Q plot
    plt.subplot(1, 3, 2)
    stats.probplot(df[variable], dist="norm", plot=plt)
    plt.ylabel('Quantiles')

    # boxplot
    plt.subplot(1, 3, 3)
    sns.boxplot(y=df[variable])
    plt.title('Boxplot')

    plt.show()
    
xtr=pd.DataFrame()

#1. Dealing with MISSING DATA
mean=x_train.total_bedrooms.mean()
xtr['tb_mean']=x_train['total_bedrooms'].fillna(mean)
median=x_train.total_bedrooms.median()
xtr['tb_median']=x_train['total_bedrooms'].fillna(median)

diagnostic_plots(x_train, 'total_bedrooms')
diagnostic_plots(xtr, 'tb_mean')
diagnostic_plots(xtr, 'tb_median')

x_train['total_bedrooms']=xtr['tb_mean']
x_test.total_bedrooms=x_test.total_bedrooms.fillna(mean)

#2. Dealing with Categorical Data

abc=list(x_train.columns)
for c in abc :
    print(c,x_train[c].nunique())
    
#ordinal encoding to have a monotonic relationship b/w the categorical variable & target variable
   
dummy=pd.get_dummies(x_train.ocean_proximity,drop_first=True)
x_train=pd.concat([dummy,x_train],axis=1)
x_train=x_train.drop(['ocean_proximity'],axis=1)
dummy=pd.DataFrame()
dummy=pd.get_dummies(x_test.ocean_proximity,drop_first=True)
x_test=pd.concat([dummy,x_test],axis=1)
x_test=x_test.drop(['ocean_proximity'],axis=1)

#3. Numerical Variable Transformation

def select_vtm(df1,df2,var):
    df2[var+'_log'] = np.log(df1[var])
    diagnostic_plots(df2, var+'_log')
    print('Logarithmic\n')
    

    df2[var+'_rec'] = 1 / (df1[var]) 
    diagnostic_plots(df2, var+'_rec')
    print('Reciprocal\n')
    
    df2[var+'_sqr'] = df1[var]**(1/2) 
    diagnostic_plots(df2,var+'_sqr')
    print('SquareRoot\n')

    df2[var+'_exp'] = df1[var]**(1/1.5) 
    diagnostic_plots(df2, var+'_exp')
    print('Exponential\n')

    df2[var+'_boxcox'], param = stats.boxcox(df1[var]) 
    print('Optimal λ: ', param)
    diagnostic_plots(df2, var+'_boxcox')
    print('Boxcox\n')

    df2[var+'_yeojohnson'], param = stats.yeojohnson(df1[var].
                                                    astype('float')) 
    print('Optimal λ: ', param)
    diagnostic_plots(df2, var+'_yeojohnson')  
    print("YeoJohnson\n")

xtr=pd.DataFrame()

diagnostic_plots(x_train, 'median_income')
select_vtm(x_train,xtr,'median_income')
x_train['median_income']=xtr['median_income_yeojohnson']
x_test['median_income'],param=stats.yeojohnson(x_test['median_income']
                                               .astype('float'))


diagnostic_plots(x_train, 'housing_median_age')

diagnostic_plots(x_train, 'total_rooms')
select_vtm(x_train,xtr,'total_rooms')
x_train['total_rooms']=xtr['total_rooms_yeojohnson']
x_test['total_rooms'],param=stats.yeojohnson(x_test['total_rooms']
                                               .astype('float'))

diagnostic_plots(x_train, 'total_bedrooms')
select_vtm(x_train,xtr,'total_bedrooms')
x_train['total_bedrooms']=xtr['total_bedrooms_yeojohnson']
x_test['total_bedrooms'],param=stats.yeojohnson(x_test['total_bedrooms']
                                               .astype('float'))

diagnostic_plots(x_train, 'population')
select_vtm(x_train,xtr,'population')
x_train['population']=xtr['population_yeojohnson']
x_test['population'],param=stats.yeojohnson(x_test['population']
                                               .astype('float'))

diagnostic_plots(x_train, 'households')
select_vtm(x_train,xtr,'households')
x_train['households']=xtr['households_yeojohnson']
x_test['households'],param=stats.yeojohnson(x_test['households']
                                               .astype('float'))
xtr=pd.DataFrame()
diagnostic_plots(y_train, 'median_house_value')
select_vtm(y_train,xtr,'median_house_value')
y_train['median_house_value']=xtr['median_house_value_yeojohnson']
y_test['median_house_value'],param=stats.yeojohnson(y_test['median_house_value']
                                                    .astype('float'))

#4. Discretization of variables
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import cross_val_score

#descritizing latitude 
scores_ls=[]
scores_ls_std=[]
xtr=pd.DataFrame()

#finding best depth of tree
for td in [1,2,3,4] :
    t_m=DecisionTreeRegressor(max_depth=td)
    scores=cross_val_score(t_m,x_train['latitude'].to_frame(),y_train,cv=5)
    scores_ls.append(np.mean(scores))
    scores_ls_std.append(np.std(scores))

xtr=pd.concat([pd.Series(scores_ls),pd.Series(scores_ls_std)],axis=1)

xtr.columns=['mean score','std_dev']

#fitting test&train set with discretized variable
tree_model=DecisionTreeRegressor(max_depth=4)
tree_model.fit(x_train['latitude'].to_frame(),y_train)
x_train['latitude_tree']=tree_model.predict(x_train['latitude'].to_frame())

diagnostic_plots(x_train,'latitude')
diagnostic_plots(x_train,'latitude_tree')

x_train=x_train.drop(['latitude'],axis=1)

x_test['latitude_tree']=tree_model.predict(x_test['latitude'].to_frame())

x_test=x_test.drop(['latitude'],axis=1)

#descritizing longitude
scores_ls=[]
scores_ls_std=[]
xtr=pd.DataFrame()

#finding best depth of tree
for td in [1,2,3,4] :
    t_m=DecisionTreeRegressor(max_depth=td)
    scores=cross_val_score(t_m,x_train['longitude'].to_frame(),y_train,cv=5)
    scores_ls.append(np.mean(scores))
    scores_ls_std.append(np.std(scores))

xtr=pd.concat([pd.Series(scores_ls),pd.Series(scores_ls_std)],axis=1)

xtr.columns=['mean score','std_dev']

#fitting test&train set with discretized variable
tree_model=DecisionTreeRegressor(max_depth=4)
tree_model.fit(x_train['longitude'].to_frame(),y_train)
x_train['longitude_tree']=tree_model.predict(x_train['longitude'].to_frame())

diagnostic_plots(x_train,'longitude')
diagnostic_plots(x_train,'longitude_tree')

x_train=x_train.drop(['longitude'],axis=1)

x_test['longitude_tree']=tree_model.predict(x_test['longitude'].to_frame())

x_test=x_test.drop(['longitude'],axis=1)

#5. Detecting Outliers

abc=list(x_train.columns)
for c in abc :
    print(c,"\n")
    diagnostic_plots(x_train,c)


def find_boundaries(df, variable):

    # the boundaries are the quantiles

    lower_boundary = df[variable].quantile(0.05)
    upper_boundary = df[variable].quantile(0.95)

    return upper_boundary, lower_boundary


ul,ll = find_boundaries(x_train, 'total_rooms')
x_train['total_rooms']= np.where(x_train['total_rooms'] > ul, ul,
                       np.where(x_train['total_rooms'] < ll, ll, x_train['total_rooms']))
diagnostic_plots(x_train,'total_rooms')
x_test['total_rooms']= np.where(x_test['total_rooms'] > ul, ul,
                       np.where(x_test['total_rooms'] < ll, ll, x_test['total_rooms']))


ul,ll = find_boundaries(x_train, 'total_bedrooms')
x_train['total_bedrooms']= np.where(x_train['total_bedrooms'] > ul, ul,
                       np.where(x_train['total_bedrooms'] < ll, ll, x_train['total_bedrooms']))
diagnostic_plots(x_train,'total_bedrooms')
x_test['total_bedrooms']= np.where(x_test['total_bedrooms'] > ul, ul,
                       np.where(x_test['total_bedrooms'] < ll, ll, x_test['total_bedrooms']))


ul,ll = find_boundaries(x_train, 'population')
x_train['population']= np.where(x_train['population'] > ul, ul,
                       np.where(x_train['population'] < ll, ll, x_train['population']))
diagnostic_plots(x_train,'population')
x_test['population']= np.where(x_test['population'] > ul, ul,
                       np.where(x_test['population'] < ll, ll, x_test['population']))

ul,ll = find_boundaries(x_train, 'households')
x_train['households']= np.where(x_train['households'] > ul, ul,
                       np.where(x_train['households'] < ll, ll, x_train['households']))
diagnostic_plots(x_train,'households')
x_test['households']= np.where(x_test['households'] > ul, ul,
                       np.where(x_test['households'] < ll, ll, x_test['households']))

ul,ll = find_boundaries(x_train, 'median_income')
x_train['median_income']= np.where(x_train['median_income'] > ul, ul,
                       np.where(x_train['median_income'] < ll, ll, x_train['median_income']))
diagnostic_plots(x_train,'median_income')
x_test['median_income']= np.where(x_test['median_income'] > ul, ul,
                       np.where(x_test['median_income'] < ll, ll, x_test['median_income']))

ul,ll = find_boundaries(x_train, 'latitude_tree')
x_train['latitude_tree']= np.where(x_train['latitude_tree'] > ul, ul,
                       np.where(x_train['latitude_tree'] < ll, ll, x_train['latitude_tree']))
diagnostic_plots(x_train,'latitude_tree')
x_test['latitude_tree']= np.where(x_test['latitude_tree'] > ul, ul,
                       np.where(x_test['latitude_tree'] < ll, ll, x_test['latitude_tree']))

ul,ll = find_boundaries(x_train, 'longitude_tree')
x_train['longitude_tree']= np.where(x_train['longitude_tree'] > ul, ul,
                       np.where(x_train['longitude_tree'] < ll, ll, x_train['longitude_tree']))
diagnostic_plots(x_train,'longitude_tree')
x_test['longitude_tree']= np.where(x_test['longitude_tree'] > ul, ul,
                       np.where(x_test['longitude_tree'] < ll, ll, x_test['longitude_tree']))


final=pd.concat([x_train,y_train],axis=1)
corr=final.corr()
ax=sns.heatmap(corr,vmin=-1,vmax=1,cmap="coolwarm",xticklabels=final.columns,
               yticklabels=final.columns)

#building optimal model using backward elimination
x_train=np.append(arr=np.ones((10320,1)).astype(int),values=x_train,axis=1)
x_test=np.append(arr=np.ones((10320,1)).astype(int),values=x_test,axis=1)

import statsmodels.api as sm
x_opt=x_train[:,[0,1,2,3,4,5,6,7,8,9,10,11,12]]
regressor_ols=sm.OLS(y_train,x_opt).fit()
regressor_ols.summary()

from sklearn.preprocessing import PolynomialFeatures
pr=PolynomialFeatures(degree=6)
x_train=pr.fit_transform(x_train)
x_test=pr.fit_transform(x_test)

from sklearn.preprocessing import StandardScaler
sc_x=StandardScaler()
sc_y=StandardScaler()
x_train=sc_x.fit_transform(x_train)
x_test=sc_x.fit_transform(x_test)
y_train=sc_y.fit_transform(y_train)
y_test=sc_y.fit_transform(y_test)

from sklearn.linear_model import Ridge
ridge=Ridge(alpha=0.001,normalize=True)
ridge.fit(x_train,y_train)
y_pred=ridge.predict(x_test)

#evaluating score
from sklearn.metrics import r2_score
print("***For FEATURED ENGINEERED & POYNOMIAL FEATURES MODEL***")
print("Train Set Accuracy = ",r2_score(y_train,ridge.predict(x_train))*100,"%")
print("Test Set Accuracy = ",r2_score(y_test,y_pred)*100,"%")
print('Mean Absolute Error = ',np.mean(abs(y_pred - y_test)))
print('Mean Squared Error = ',np.mean((y_pred - y_test)**2))
print('Root Mean Squared Error = ',(np.mean((y_pred - y_test)**2))**0.5)

y_test=sc_y.inverse_transform(y_test)
y_pred=sc_y.inverse_transform(y_pred)
ax1=sns.distplot(y_test,hist=False,color='r',label='Test Set')
sns.distplot(y_pred,hist=False,color='b',label='Predicted Values',ax=ax1)